
\documentclass[10pt,conference]{IEEEtran}


\usepackage{url}
\usepackage{color}
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{AlphaDiff}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
%\author{\IEEEauthorblockN{X}
%\IEEEauthorblockA{X}}
%\and
%\IEEEauthorblockN{Homer Simpson}
%\IEEEauthorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
%\IEEEauthorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
% \author{\IEEEauthorblockN{Author\IEEEauthorrefmark{1},
% Author2\IEEEauthorrefmark{2},
% Author3\IEEEauthorrefmark{3}, 
% Author4\IEEEauthorrefmark{3} and
% Author5\IEEEauthorrefmark{4}}
% \IEEEauthorblockA{\IEEEauthorrefmark{1}School\\
% University,
% address\\ Email: email@email.com}
% \IEEEauthorblockA{\IEEEauthorrefmark{2}School\\
% University,
% address\\ Email: email@email.com}
% \IEEEauthorblockA{\IEEEauthorrefmark{3}School\\
% University,
% address\\ Email: email@email.com}
% \IEEEauthorblockA{\IEEEauthorrefmark{4}School\\
% University,
% address\\ Email: email@email.com}}


\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
The abstract goes here.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
Finding the difference between two binaries is a key problem in many security analysis settings, 
such as patch analysis, malware analysis and vulnerability search. 
For example, in patch analysis, the attacker makes use of a pre-patched binary and a post-patched binary to find the patched 
vulnerabilities \cite{brumley2008automatic} or unknown vulnerabilities with a similar pattern 
to the patched \cite{xu2017spain}. 
In malware analysis, the researchers analyze the similarity between different malware samples to classify 
them \cite{bayer2009scalable}\cite{hu2009large} or infer lineage relations \cite{jang2013towards}. 
In vulnerability search, the researchers search for similar functions to a vulnerable one with the goal of find new vulnerabilities
 \cite{eschweiler2016discovre}\cite{feng2016scalable}\cite{xu2017neural}.

Binary “diffing” is the inherent challenge shared by the above applications. 
BinDiff \cite{bindiff} is the current state-of-the-art industrial binary diffing tool, whose core approach is based on graph-isomorphism (GI) 
theory \cite{Dullien2005}\cite{flake2004structural} to find similarities and differences between binaries. 
Inspired by BinDiff, BinHunt \cite{gao2008binhunt} and iBinHunt \cite{ming2012ibinhunt} furtherly apply symbolic 
execution and theorem proving to check functional equivalence among basic blocks to find semantic differences between functions. 
Based on the idea of “Similarity of Composition” instead of control flow graph (CFG) isomorphism, BinGo \cite{chandramohan2016bingo}
 and Esh \cite{david2016statistical} statistically reason similarity of whole functions using smaller fragments similarities that 
 are semantically equivalent scores computed by a SMT solver or a program verifier. 
In contrast to the aforementioned static methods, BLEX \cite{egele2014blanket} and IMF-SIM \cite{wang2017memory} iteratively execute each 
 function from two input binaries under controlled randomized environment and extract runtime behavior features to furtherly 
 determine the similarity among them. They mainly focus on binary diffing problem in the cross-compiler and cross-optimization-level setting. 

All known static binary diffing methods work on assembly code level. 
Some of them such as BinDiff, DiscovRE and Genius are mainly based on CFG graph matching and have some inevitable drawbacks. 
First, such approaches are built on the assumption that two semantically equivalent functions have similar CFGs, which can be easily invalidated 
by common change to the compiler’s optimization level or CPU architecture. 
For example, the accuracy of BinDiff can be reduced to 0.25 when diffing two binaries compiled from the same source code with -O0 and -O3 
respectively \cite{egele2014blanket}; 
Second, the graph isomorphism problem has no known polynomial time algorithm, i.e. accurate algorithms are inevitably inefficient and 
actually the state of the art for the maximum common subgraph problem becomes computationally infeasible at only 35 vertices when 
working with unlabeled graphs \cite{hoffmann2017between}.
Although carefully designed heuristic algorithms such as BinDiff’s are usably fast, they are greedy and erroneous matches will 
propagate through the isomorphism process; 
third, these approaches ignore the semantics of concrete assembly-level instructions, as a result, they can’t find the differences 
due to pure dataflow patches, e.g. increasing the related buffer size to patch a buffer overflow vulnerability. 
Some other approaches such as BinHunt, BinGo and Esh take instructions’ semantics into consideration, however, they rely on SAT/SMT 
solvers to determine semantic equivalence, which are computationally expensive. 
For example, Esh needs on average 3 minutes to compare a pair of functions on an 8-core Ubuntu machine \cite{david2016statistical}. 
Besides, dynamic approaches such as BLEX and Esh rely on architecture-speciﬁc tools to execute or emulate binaries, as a result, 
they are inconvenient to apply, especially when diffing binaries that are from different CPU architectures, even firmware images, 
or with destructive semantics e.g., rm of Coreutils.

In recent years, deep learning \cite{lecun2015deep} has been applied to many domains, include binary analysis \cite{shin2015recognizing}, 
and show stronger results than other approaches. 
Inspired by DiscovRE and Genius, Gemini \cite{xu2017neural} assumes each function can be represented as an ACFG, which is a CFG with attributes 
attached to each node, and there should exist a function mapping similarities of ACFGs to similarities of corresponding functions; 
finally, it chooses to use deep learning and represents the function as a deep neural network model which encodes each ACFG into a 
measurable embedding and the distances can reflect the similarities of functions.
However, Gemini still relies on CFG structure and a set of numeric features, both of which are “hand-tuned” features, and probably 
pose obstacles while bringing benefits at a more abstract level. Besides, it also ignore the semantics of concrete assembly-level instructions.

Inspired by the work of Shin et al. \cite{shin2015recognizing}, we propose a novel deep learning-assisted static approach to extract features for 
diffing two versions of the same executable (stripped) on the {\textbf{machine code}} level. 
Machine code of a function contains all information one can fetch from its assembly code, including instructions’ semantics. 
Although machine code is so unreadable that it’s hard for humans to directly extract features from it for diffing, 
it is much more readable to deep neural network. 
Diffing two versions of the same binary is the initial problem to be solved in this research domain \cite{wang2000bmat} and 
also the main objective of BinDiff; 
since BinDiff was released, most of researchers think that there is no more need to study it and turn attention to binary 
diffing in the cross-compiler and cross-architecture settings, however, the average match accuracy of BinDiff is less than {\color{red}XX} 
on our benchmark consisting of {\color{red}X} pairs of binaries. 
We aim to reconstruct the solutions of this domain starting from the initial problem.

Extracting diffing features on the machine code level is changeling. 
Due to the NP-complete nature of the compiler optimization problem \cite{aho2003compilers}, 
even recompiling the same source code with the same compiler and optimization options can potentially alter the resulting binary, 
e.g. register re-allocation and instruction/basic block re-ordering that will finally be represented on the machine code. 
This become more complicated when diffing different versions of the same binaries, and even further in cross-compiler (vendor, version, optimization level) 
or cross-architecture setting.

The approach takes two stripped binaries as inputs, each of which can be viewed as a set of functions, and outputs the (partial) matching with 
similarity/distance between elements in the two sets. 
To this end, we make use of 3 features: intra-function feature, inter-function feature and inter-module feature. 
The first one is identified by deep neural network on the machine code level, and the last two are identified by human because our 
dataset can’t represent the global features of one function. 
For intra-function feature, we represent a function as a matrix whose elements are the machine code in bytes and use convolutional 
neural network (CNN) to convert the matrix into an embedding, i.e. a vector. Furtherly, in order to make the embedding have desirable 
properties and place similar items nearby and dissimilar ones apart from each other, we embed the CNN into a Siamese network, 
a deep metric learning network that has been used for fine-grained visual similarity recognition
 \cite{bromley1994signature}\cite{bell2015learning}\cite{schroff2015facenet}\cite{song2016deep}.
For inter-function feature, we encode the in-degree and out-degree of a function in the call graph (CG) generated for the entire binary 
file into a vector, to describe the function from the view of relationship with the other functions. 
For inter-module feature, we encode the calls of a function to common imported functions between two binaries into a one-hot vector, 
to describe the function from the view of relationship with imported functions that are defined in other files. 
The inter-function feature and inter-module feature can be treated together as a global context feature. 
{\color{red}Finally, we separately compute a distance for each feature and merge the three distances to measure the similarity of two functions.} 

We have implemented a prototype of our approach, named $\alpha$Diff.
{\color{red}{In our evaluation on a small benchmark consisting of {\color{yellow}{XX}} pairs of binaries, $\alpha$Diff outperforms BinDiff by {\color{yellow}{XX}} percentage on average. 
We separately evaluate the intra-function feature on the testset containing 9308 pairs of binaries and 1,408,297 user-defined functions, and accuracy 
of top-1 is 0.924 on average and accuracy of top-5 is 0.992 on average. 
Besides, we apply the identified features to diff binaries in cross-compiler (vendor, version, optimization level) and cross-architecture settings 
and show that $\alpha$Diff outperforms the state-of-the-art static approach by {\color{yellow}{XX}} percentage on average.}}

Overall, our main contributions are as follows:
\begin{enumerate}
\item We first propose a deep learning-assisted approach (Intelligence augmentation, IA) to identify diffing features for cross-binary-version on the machine code level, 
and show that it performs better than the state-of-the-art industrial tool BinDiff in our evaluation.

\item We have built a labelled dataset for deep learning, which contains 66,823 pairs of binaries and more than 2.5 million pairs of functions. 
Researchers can freely use the dataset to discover better neural network models.

\item We apply the features that are identified for the cross-version setting to diff binaries in the cross-architecture and cross-compiler settings, 
and shows that alphaDiff performs better than the state-of-the-art static approach specialized in these.
\end{enumerate}
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)

\section{Motivation \& Problem Definition}

\section{Approach}

\subsection{Siamese network}

\subsection{CNN design}

\subsection{Global context features}

\section{Evaluation}

\subsection{Implementation and Setup}
We have implement a prototype of our approach named $\alpha$Diff, consisting of two
 components: inputs extractor and neural network model. 
The inputs extractor is a IDAPython \cite{idapython} plug-in to the disassembly tool
 IDA Pro \cite{idapro} and is used to extract the machine code, in/out-degree in call
  graph and calls to imported functions of a function. 
We implement the neural network model with TensorFlow-1.3 \cite{abadi2016tensorflow}
 and Keras-2.0 \cite{chollet2015keras}. 
Our experiments are conducted on a server equipped with two Intel Xeon E5-2650v4
 CPUs (24 cores in total) running at 2.20 GHz, 128 GB memory, 12TB hard drives,
  and 4 NVIDIA Tesla P100 PCIE-16G GPU cards. 
During both training and evaluation, only 1 GPU card was used.

\textbf{Datasets.} In our evaluation, we collect four datasets: 
(1) Dataset \uppercase\expandafter{\romannumeral1} for training the neural network, evaluating the accuracy of the pre-trained model
 and evaluating the effectiveness of hyperparameters in our model; 
(2) Dataset \uppercase\expandafter{\romannumeral2} for evaluating the performance of the model in real-world cross-version settings; 
(3) Dataset \uppercase\expandafter{\romannumeral3} for evaluating the performance in cross-compiler settings; 
(4) Dataset \uppercase\expandafter{\romannumeral4} for evaluating the performance in cross-architecture settings.
\begin{enumerate}
\item \textbf{Dataset \uppercase\expandafter{\romannumeral1}.} 
  The dataset is used to train neural network model and evaluate the accuracy of the pre-trained model. 
  The dataset is consisted of 66,823 pairs of binaries and each of them are two versions of the same
   executable. 
  These binaries comes from two sources:
   (1) GitHub, we collect the releases of some open source projects on GitHub, such as Vim and Git;
    then, we compile them into executables and group two binaries with adjacent version numbers into a pair;
     finally, we obtain 23,524 pairs of binaries; 
  (2) Ubuntu repositories, we collect binaries and their corresponding debug symbol files that coexist on
   Ubuntu 12.04, 14.04 and 16.04; then, we group them into pairs in the similar way. 
   All of these binaries have debug symbols, so that they can provide the ground truth for us. 
   Besides, they are all in x86. We extract samples from the binaries with Inputs Extractor and each sample 
   contains a function’s machine code, in/out-degree in CG and calls to imported functions. 
   For each pair of binaries, we extract machine code samples corresponding to all of changed functions and a small part
    of unchanged ones between two versions. In total, we obtain 2,489,793 samples (pairs of functions) and
     70.6\% of them are function pairs with source code differences. 
     We split Dataset \uppercase\expandafter{\romannumeral1} into three disjoint sets of binary pairs for training (44,526 pairs of binaries, 1,665,025 pairs of functions),
      validation (11,150 pairs of binaries, 417,158 pairs of functions) and testing (11,147 pairs of binaries, 407,610 pairs of functions)
       respectively. During the split, 
    we guarantee that no two binaries from the same project are separated into two different sets among
     training, validation and testing. 
    By doing so, we can examine whether the pre-trained model can generalize to unseen binary files.  
\item \textbf{Dataset \uppercase\expandafter{\romannumeral2}.}
In order to evaluate the performance of $\alpha$Diff in real-world cross-version settings, we construct two sub-datasets:
 (1) We re-extract all function samples including machine code, in/out-degree in CG and calls to imported functions for each binary pair in the testing set of Dataset \uppercase\expandafter{\romannumeral1},
 because we oly extract a portion of functions when constructing Dataset \uppercase\expandafter{\romannumeral1} which is different from the real-world scenarios. 
 In total, the sub-dataset contains 9308 pairs of binaries and more than 1.4 Million user-defined functions.
 (2) In order to compare with BinDiff in classic cross-version setting, we tried to search for standard benchmarks used by other researchers, however,
  we didn't find anyone. So we build a open benchmark which consists of binaries hard to diff and none of the binaries exists in Dataset \uppercase\expandafter{\romannumeral1}.
  Finally, the sub-dataset contains 400 pairs of binaries and 365,374 functions including 176,524 user-defined functions.

\item \textbf{Dataset \uppercase\expandafter{\romannumeral3}.}

\item \textbf{Dataset \uppercase\expandafter{\romannumeral4}.}

\end{enumerate}

\textbf{Training details.} The neural network model is trained using Dataset \uppercase\expandafter{\romannumeral1} as follow. 
We use RMSProp optimizer ...

\subsection{Accuracy}
In this section, we evaluate the accuracy of the pre-trained neural network model (Siamese network embedded with CNN) and $\alpha$Diff in Dataset \uppercase\expandafter{\romannumeral2}.
Before evaluating, we briefly introduce our evaluation metric used in the experiments. 
For two given binaires $BIN_{pre}$ and $BIN_{post}$, not every function in $BIN_{pre}$ has a match in $BIN_{post}$; 
so we use the \textbf{Recall@K} metric \cite{jegou2011product}\cite{song2016deep}. 
Each function (query) in $BIN_{pre}$ first retrieves K nearest neighbors in $BIN_{post}$ and receives 1 if the matched function (with the same function name) is retrieved among the K nearest
 neighbors and 0 otherwise. 
Slightly different to \cite{jegou2011product}\cite{song2016deep}, we furtherly divide the Recall@K number by the total number of matches with same function name between $BIN_{pre}$ and $BIN_{post}$
 and use the result as \textbf{Recall@K} metric.
We evaluate the accuracy of pre-train neural network model (only intra-function feature) and $\alpha$Diff (all three features) respectively in Dataset \uppercase\expandafter{\romannumeral2}-1.
 For each binary pair, we compute Recall@1 and Recall@5 respectively. Table \uppercase\expandafter{\romannumeral1} shows the final statistics result.

  \begin{table}[!t]
  \caption{改变表格任一列宽}
  \begin{tabular}{p{2cm}|p{3cm}|p{3cm}}  
  \hline  
  \hline  
   & Siamese network \textbf{(intra-function feature)} & $\alpha$Diff \textbf{(all three features)} \\  
  \hline
  Recall@1 mean & 0.926 & 0.929 \\  
  \hline  
  Recall@5 mean & 0.993 & 0.994 \\   
  \hline  
  \hline  
  \end{tabular}  
  \end{table} 

   

top-1 accuracy in testing set and the selected smaller benchmark VS BinDiff

\subsection{Hyperparameters}
\subsubsection{the number of negative samples corresponding to a positive sample}

1:1 vs 1:3 vs 1:7 vs 1:15

\subsubsection{embedding size}

64 vs 128 vs 256 vs 512 vs 1024

\subsubsection{distance metric}

euclidean distance vs cosine distance

\subsubsection{pooling algorithm}
max pooling vs average pooling

\subsection{Compare to other works}

\par To Esh \& BinDiff in cross-compiler

\par To BinGo in cross-compiler \& cross-architecture

\par To Genius \& Gemini in cross-architecture vulnerability search





\section{Discussion}

\section{Related Work}

\subsection{deep learning}

\subsection{binary diffing}



\section{Conclusion}
The conclusion goes here.


% \section*{Acknowledgment}
% The authors would like to thank...


\bibliographystyle{IEEEtran}
\bibliography{ref}
\end{document}


